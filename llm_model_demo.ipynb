{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5fce3c",
   "metadata": {},
   "source": [
    "### Notebook On Understanding How A Large Language Model Works"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4de2c207",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The capital of India is ____ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cce14",
   "metadata": {},
   "source": [
    "##### 1. Text is converted into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f4591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data Engineering is about preparing data for analytical workload.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b00bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0933ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'Engineering',\n",
       " 'is',\n",
       " 'about',\n",
       " 'preparing',\n",
       " 'data',\n",
       " 'for',\n",
       " 'analytical',\n",
       " 'workload.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bce89",
   "metadata": {},
   "source": [
    "#### 2. Tokens are mapped to embeddings (vectors)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e4fdb4a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"car\"  → [0.12, -0.45, 0.89, ...]\n",
    "\"truck\"→ [0.10, -0.40, 0.92, ...]\n",
    "\"apple\"→ [-0.91, 0.22, -0.13, ...]\n",
    "\n",
    "\n",
    "'Data' -> [0.25, -0.67, 0.34, 0.035, ...]\n",
    " 'Engineering'-> [0.30, 0.70, 0.40, 0.045, ...]\n",
    " 'is'->[0.15, -0.20, 0.10, 0.005, ...]\n",
    " 'about'->[0.28, -0.60, 0.38, 0.030, ...]\n",
    " 'preparing'   ->[0.32, -0.75, 0.42, 0.050, ...]\n",
    " 'data'->\n",
    " [0.25, -0.67, 0.34, 0.035, ...]\n",
    " 'for'->\n",
    " [0.18, -0.50, 0.22, 0.020, ...]\n",
    " 'analytical'  -> \n",
    " [0.29, -0.62, 0.36, 0.028, ...]\n",
    " 'workload.'->\n",
    " [0.22, -0.55, 0.30, 0.025, ...]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4095f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car vs truck: 0.9979517409161514\n",
      "car vs banana: -0.9374252720097651\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "car = [0.9, 0.1]\n",
    "truck = [0.85, 0.15]\n",
    "banana = [-0.8, 0.2]\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot = sum(a*b for a,b in zip(v1,v2))\n",
    "    mag1 = math.sqrt(sum(a*a for a in v1))\n",
    "    mag2 = math.sqrt(sum(b*b for b in v2))\n",
    "    return dot / (mag1 * mag2)\n",
    "\n",
    "print(\"car vs truck:\", cosine_similarity(car, truck))\n",
    "print(\"car vs banana:\", cosine_similarity(car, banana))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2195b7",
   "metadata": {},
   "source": [
    "Step 3: Transformer Processes Context Using Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750163f",
   "metadata": {},
   "source": [
    "I deposited money in the bank. bank > financial institution {deposited, money}\n",
    "\n",
    "\n",
    "I sat near the river bank. > Natural landscape { river bank }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde296c9",
   "metadata": {},
   "source": [
    "####  Demo On Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba86f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Attention Weights:\n",
      " [[0.57611688 0.21194156 0.21194156]\n",
      " [0.21194156 0.57611688 0.21194156]\n",
      " [0.21194156 0.21194156 0.57611688]]\n",
      "\n",
      "Final Representations:\n",
      " [[0.78805844 0.42388312 0.78805844]\n",
      " [0.78805844 0.78805844 0.42388312]\n",
      " [0.42388312 0.78805844 0.78805844]]\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# word embeddings (fake)\n",
    "words = [\"I\", \"love\", \"data\"]\n",
    "embeddings = np.array([\n",
    "    [1, 0, 1],   # I\n",
    "    [1, 1, 0],   # love\n",
    "    [0, 1, 1]    # data\n",
    "])\n",
    "\n",
    "# Query, Key, Value = same (self-attention)\n",
    "Q = embeddings\n",
    "K = embeddings\n",
    "V = embeddings\n",
    "\n",
    "# Attention scores\n",
    "scores = Q @ K.T\n",
    "\n",
    "# Softmax\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "weights = softmax(scores)\n",
    "\n",
    "# Output\n",
    "output = weights @ V\n",
    "\n",
    "print(\"Attention Weights:\\n\", weights)\n",
    "print(\"\\nFinal Representations:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44442753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'Engineering',\n",
       " 'is',\n",
       " 'about',\n",
       " 'preparing',\n",
       " 'data',\n",
       " 'for',\n",
       " 'analytical',\n",
       " 'workload.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78467e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data engineering is'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Data engineering is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0457e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data engineering is fun\n",
      "Data engineering is important\n",
      "Data engineering is boring\n",
      "Data engineering is important\n",
      "Data engineering is important\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "next_token_probs = {\n",
    "    \"Data engineering is\": [\n",
    "        (\"important\", 0.45),\n",
    "        (\"challenging\", 0.30),\n",
    "        (\"fun\", 0.15),\n",
    "        (\"boring\", 0.10)\n",
    "    ]\n",
    "}\n",
    "\n",
    "def predict_next(text):\n",
    "    words, probs = zip(*next_token_probs[text])\n",
    "    return random.choices(words, probs)[0]\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"Data engineering is\", predict_next(\"Data engineering is\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278a482",
   "metadata": {},
   "source": [
    "#### bedrock architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10c5a1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Client (App / Notebook / Lambda)\n",
    "        |\n",
    "        v\n",
    "AWS SDK / API (InvokeModel)\n",
    "        |\n",
    "        v\n",
    "-----------------------------\n",
    "| Amazon Bedrock Service   |\n",
    "|--------------------------|\n",
    "| Control Plane            |\n",
    "| Runtime Plane            |\n",
    "-----------------------------\n",
    "        |        \n",
    "        v\n",
    "Foundation Models\n",
    "(Claude, Llama, Titan, etc.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d87a188f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Prompt:\n",
    "\n",
    "\n",
    "You are an AI assistant helping a CTO.\n",
    "\n",
    "Task:\n",
    "Summarize the following incident report in exactly 5 bullet points.\n",
    "\n",
    "\n",
    "Incident:\n",
    "\n",
    "On 12 Jan 2026, FinSure detected unauthorized access to its production environment caused by compromised employee credentials. The attacker exfiltrated sensitive customer data over a 6-hour window.\n",
    "08:45 AM\n",
    "A data analyst receives a phishing email disguised as a Microsoft Teams security alert.\n",
    "\n",
    "09:02 AM\n",
    "The analyst enters credentials on a fake login page.\n",
    "→ Credentials are harvested.\n",
    "\n",
    "09:18 AM\n",
    "Attacker logs in via VPN from an unusual IP (Eastern Europe).\n",
    "\n",
    "09:25 AM\n",
    "Attacker accesses:\n",
    "\n",
    "AWS S3 bucket containing customer risk reports\n",
    "\n",
    "Read-only access escalated using misconfigured IAM role\n",
    "\n",
    "11:40 AM\n",
    "~2.4 GB of data downloaded (CSV & Parquet files)\n",
    "\n",
    "03:10 PM\n",
    "CloudWatch anomaly alert triggers on abnormal data egress.\n",
    "\n",
    "03:25 PM\n",
    "Security team disables the compromised account and revokes IAM tokens.\n",
    "\n",
    "\n",
    "\n",
    "Constraints:\n",
    "- Use professional tone\n",
    "- Highlight risks and action items\n",
    "- Do not add assumptions\n",
    "- Use only the provided information\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1bc427b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Output From AWS Nova :\n",
    "\n",
    "* On 12 Jan 2026, FinSure experienced unauthorized access to its production environment due to compromised employee credentials, resulting in the exfiltration of sensitive customer data.\n",
    "* The incident initiated at 08:45 AM when a data analyst fell victim to a phishing email disguised as a Microsoft Teams security alert, leading to credential harvesting at 09:02 AM.\n",
    "* At 09:18 AM, the attacker accessed the system via VPN from an unusual Eastern European IP, subsequently gaining access to an AWS S3 bucket containing customer risk reports by 09:25 AM.\n",
    "* Approximately 2.4 GB of data, including CSV and Parquet files, was downloaded by the attacker over a 6-hour window, concluding at 11:40 AM.\n",
    "* A CloudWatch anomaly alert at 03:10 PM led to the security team's prompt action at 03:25 PM, disabling the compromised account and revoking IAM tokens to mitigate further damage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2230c94",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Output From Claude (Sonnet 4.5):\n",
    "\n",
    "\n",
    "Incident Summary – 12 January 2026\n",
    "\n",
    "• Breach Overview: FinSure experienced unauthorized access to its production environment resulting in the exfiltration of approximately 2.4 GB of sensitive customer risk reports (CSV and Parquet files) from an AWS S3 bucket over a 6-hour period.\n",
    "\n",
    "• Attack Vector: A data analyst fell victim to a phishing email impersonating a Microsoft Teams security alert, resulting in credential compromise at 09:02 AM; the attacker subsequently accessed the VPN from an Eastern European IP address and leveraged a misconfigured IAM role to escalate from read-only to broader access.\n",
    "\n",
    "• Detection Gap: The breach occurred between 09:18 AM (initial unauthorized VPN login) and 03:10 PM (CloudWatch anomaly detection), representing a ~6-hour window where the attacker operated undetected within the production environment.\n",
    "\n",
    "• Containment Actions: The security team disabled the compromised account and revoked IAM tokens at 03:25 PM, approximately 15 minutes after the anomaly alert triggered, successfully terminating the attacker's access.\n",
    "\n",
    "• Critical Risk Factors: Key vulnerabilities include insufficient phishing awareness training, lack of multi-factor authentication enforcement on VPN access, misconfigured IAM roles enabling privilege escalation, and delayed detection of abnormal data egress patterns—all requiring immediate remediation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28087b87",
   "metadata": {},
   "source": [
    "#### Basics Of Prompt Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0617c465",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "prompt = 'You are a Lead Data Engineer.Explain ETL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bc2df",
   "metadata": {},
   "source": [
    "Strategies\n",
    "\n",
    "* Act like someone -> assign a persona\n",
    "* Better or Detailed Context -> \n",
    "* Automated Incorporation Of Context ->\n",
    "* Giving Examples -> Few-Shot prompting\n",
    "* Tone -> \n",
    "* Evaluate the results before it gives an output -> Giving the model more time to think and process the output before it is returned. [Avoid Hallucination]\n",
    "* Chain-Of-Thought prompting -> \n",
    "* Constraint -> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcbc1f",
   "metadata": {},
   "source": [
    "What comprises a good prompt ?\n",
    "\n",
    "\n",
    "Instruction + Context + Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain ETL to a beginner' # Instruction + context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain ETL to a beginner in less than 30 words ' # Instruction + context + constraints"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3491e5c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "You are a data engineer working for an e-commerce company. -> Context\n",
    "Explain how an ETL pipeline works for daily sales data. -> Instruction\n",
    "Keep the explanation under 150 words and make it suitable for a non-technical manager. -> Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05664d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "\n",
    "def invoke_llama_model(prompt,temperature=0.5,max_tokens=512):\n",
    "\n",
    "    # Set the model ID, e.g., Llama 3 70b Instruct.\n",
    "\n",
    "    model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "    # Embed the prompt in Llama 3's instruction format.\n",
    "    formatted_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the request payload using the model's native structure.\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"generation\"]\n",
    "    \n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34922eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"Explain ETL in simple terms for a junior data engineer with Python knowledge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e312d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = invoke_llama_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854dc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As a junior data engineer with Python knowledge, you're already halfway there! ETL is a fundamental concept in data engineering, and I'm happy to explain it in simple terms.\n",
      "\n",
      "**What is ETL?**\n",
      "\n",
      "ETL stands for Extract, Transform, Load. It's a process used to move data from multiple sources to a centralized location, such as a database or data warehouse, in a standardized format. Think of it like a pipeline that helps you collect, clean, and prepare data for analysis or other uses.\n",
      "\n",
      "**The Three Steps of ETL:**\n",
      "\n",
      "1. **Extract**: This is the first step, where you \"extract\" or collect data from various sources, such as:\n",
      "\t* Databases (e.g., MySQL, PostgreSQL)\n",
      "\t* Files (e.g., CSV, JSON, Excel)\n",
      "\t* APIs (e.g., Twitter, Facebook)\n",
      "\t* Other systems (e.g., CRM, ERP)\n",
      "\n",
      "You'll use Python libraries like `pandas`, `sqlalchemy`, or `requests` to connect to these sources and retrieve the data.\n",
      "\n",
      "2. **Transform**: In this step, you \"transform\" the extracted data into a standardized format, making it consistent and usable for analysis. This may involve:\n",
      "\t* Data cleaning (e.g., handling missing values, removing duplicates)\n",
      "\t* Data aggregation (e.g., grouping, summarizing)\n",
      "\t* Data formatting (e.g., converting data types, renaming columns)\n",
      "\t* Applying business logic or rules to the data\n",
      "\n",
      "Python libraries like `pandas` and `numpy` are perfect for data transformation tasks.\n",
      "\n",
      "3. **Load**: Finally, you \"load\" the transformed data into a target system, such as:\n",
      "\t* A relational database (e.g., MySQL, PostgreSQL)\n",
      "\t* A data warehouse (e.g., Amazon Redshift, Google BigQuery)\n",
      "\t* A NoSQL database (e.g., MongoDB, Cassandra)\n",
      "\t* A file system (e.g., CSV, Parquet)\n",
      "\n",
      "You'll use Python libraries like `sqlalchemy` or `pandas` to load the data into the target system.\n",
      "\n",
      "**Why ETL is important:**\n",
      "\n",
      "ETL is crucial because it helps you:\n",
      "\n",
      "* Integrate data from multiple sources into a single, unified view\n",
      "* Ensure data consistency and quality\n",
      "* Improve data accessibility and usability\n",
      "* Support business intelligence, analytics, and reporting\n",
      "\n",
      "**Real-world example:**\n",
      "\n",
      "Imagine you're a data engineer at an e-commerce company. You need to analyze sales data from multiple sources, including a MySQL database,\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "041b128e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Task 1: Fix a Bad Prompt (Foundation)\n",
    "\n",
    "Objective: Learn why vague prompts fail.\n",
    "\n",
    "Given Prompt\n",
    "Explain data pipelines\n",
    "\n",
    "Task\n",
    "\n",
    "Improve the prompt so that:\n",
    "\n",
    "Target audience is junior data engineers\n",
    "\n",
    "Output is actionable\n",
    "\n",
    "Response length is under 120 words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1275d07c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Task 2: Role-Based Prompting\n",
    "\n",
    "Objective: Control tone, depth, and relevance.\n",
    "\n",
    "Task\n",
    "\n",
    "Generate three outputs for the same question:\n",
    "\n",
    "Explain data modeling\n",
    "\n",
    "Roles\n",
    "\n",
    "* Senior Data Engineer\n",
    "\n",
    "* Cloud Architect\n",
    "\n",
    "* Business Analyst\n",
    "\n",
    "Constraint\n",
    "\n",
    "* Max 5 bullet points\n",
    "\n",
    "* No jargon for Business Analyst\n",
    "\n",
    "Expected Outcome\n",
    "\n",
    "✔ Role changes what the model prioritizes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9debc40e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Task 3: Constraint-Driven Output Control\n",
    "\n",
    "Objective: Learn precision prompting.\n",
    "\n",
    "Task\n",
    "\n",
    "Write a prompt to:\n",
    "\n",
    "Explain Slowly Changing Dimensions\n",
    "\n",
    "Exactly 5 bullet points\n",
    "\n",
    "Each bullet ≤ 15 words\n",
    "\n",
    "No examples\n",
    "\n",
    "Evaluation\n",
    "\n",
    "Participants check if all constraints are met."
   ]
  },
  {
   "cell_type": "raw",
   "id": "705f26fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
