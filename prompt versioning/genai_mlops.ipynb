{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a894abd",
   "metadata": {},
   "source": [
    "### Token Management\n",
    "\n",
    "\n",
    "Without limits:\n",
    "User: \"Tell me about...\"\n",
    "AI: Generates 2000 tokens = $0.024\n",
    "\n",
    "With max_tokens=500:\n",
    "User: \"Tell me about...\"\n",
    "AI: Generates 500 tokens = $0.006\n",
    "\n",
    "Savings: 75% per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4463b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough estimate: ~4 characters per token\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def truncate_context(messages, max_tokens=4000):\n",
    "    \"\"\"Keep conversation under token limit\"\"\"\n",
    "    total_tokens = sum(estimate_tokens(m['content']) for m in messages)\n",
    "    \n",
    "    while total_tokens > max_tokens and len(messages) > 1:\n",
    "        # Remove oldest messages (keep last ones)\n",
    "        messages.pop(0)\n",
    "        total_tokens = sum(estimate_tokens(m['content']) for m in messages)\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39439e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Caching\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "response_cache = {}\n",
    "\n",
    "def get_cached_or_generate(query):\n",
    "    # Create hash of query\n",
    "    query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    # Check cache\n",
    "    if query_hash in response_cache:\n",
    "        print(\"Cache hit!\")\n",
    "        return response_cache[query_hash]\n",
    "    \n",
    "    # Generate new response\n",
    "    response = call_bedrock(query)\n",
    "    \n",
    "    # Cache it\n",
    "    response_cache[query_hash] = response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e5efe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Semantic Caching\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      5\u001b[39m model = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_similar_cached_query\u001b[39m(new_query, threshold=\u001b[32m0.9\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Semantic Caching\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_similar_cached_query(new_query, threshold=0.9):\n",
    "    \"\"\"Find if we've answered a similar question before\"\"\"\n",
    "    new_embedding = model.encode(new_query)\n",
    "    \n",
    "    for cached_query, response in cache.items():\n",
    "        cached_embedding = model.encode(cached_query)\n",
    "        similarity = cosine_similarity(new_embedding, cached_embedding)\n",
    "        \n",
    "        if similarity > threshold:\n",
    "            return response  # Reuse cached response\n",
    "    \n",
    "    return None  # No match, need to generate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bf7d939",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Rag Acts As A Cost Optimizer\n",
    "\n",
    "Without RAG\n",
    "============================================\n",
    "User: \"What's our refund policy?\"\n",
    "System sends: Entire conversation history + query\n",
    "Tokens: 2000 input + 300 output = $0.012\n",
    "\n",
    "\n",
    "\n",
    "With RAG\n",
    "============================================\n",
    "User: \"What's our refund policy?\"\n",
    "System retrieves: Only relevant policy doc (200 tokens)\n",
    "System sends: 200 context + query\n",
    "Tokens: 250 input + 300 output = $0.005"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bba9072",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
